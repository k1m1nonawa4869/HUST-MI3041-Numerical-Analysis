{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tìm nghiệm của đa thức $f(x) = \\sum\\limits_{i=0}^n a_i x^i = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(coefficients, x):\n",
    "    return sum(c * x**i for i, c in enumerate(coefficients))\n",
    "\n",
    "def polynomial_derivative(coefficients, x):\n",
    "    return sum(i * c * x**(i-1) for i, c in enumerate(coefficients) if i > 0)\n",
    "\n",
    "def polynomial_second_derivative(coefficients, x):\n",
    "    return sum(i * (i-1) * c * x**(i-2) for i, c in enumerate(coefficients) if i > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bán kính nghiệm\n",
    "\n",
    "* Bán kính phức: $R = 1 + \\dfrac{\\max |a_i|}{|a_n|}$\n",
    "\n",
    "* Cận trên của các nghiệm dương: Với $a_n > 0$, $a_k < 0$ (k lớn nhất có thể): $R_{upper} = 1 + \\sqrt[n-k] {\\dfrac{\\max\\limits_{a_i < 0} |a_i|}{|a_n|}}$\n",
    "\n",
    "* Cận dưới của các nghiệm âm $f(x) = 0$ $\\Leftrightarrow$ Cận trên của các nghiệm dương $f(-x) = 0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_radius(coefficients):\n",
    "    R = 1 + max(abs(coeff) for coeff in coefficients) / abs(coefficients[-1])\n",
    "    return R\n",
    "\n",
    "def real_radius(coefficients):\n",
    "    n = len(coefficients) - 1\n",
    "    negative_coefficients = [(i, abs(x)) for i, x in enumerate(coefficients) if x<0]\n",
    "    k = (max(negative_coefficients, key=lambda item: item[0]))[0]\n",
    "    ratio = (max(negative_coefficients, key=lambda item: item[1]))[1] / abs(coefficients[-1])\n",
    "    \n",
    "    R = 1 + ratio**(1/(n-k)) if n!=k else 0\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex radius for roots: -7.0 -> 7.0\n",
      "Upper radius for real-value roots: 3.449489742783178\n",
      "Lower radius for real-value roots: -7.0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "#coefficients = [1, -3, -3, 1]  # Coefficients of the polynomial 1 - 3x - 3x^2 + x^3\n",
    "coefficients = [1, -1, -6, 4, 1]\n",
    "temp = [x if i%2==0 else -x for i, x in enumerate(coefficients)]\n",
    "if (temp[-1] < 0):\n",
    "    revert_coefficients = [-i for i in temp]\n",
    "else:\n",
    "    revert_coefficients = temp\n",
    "\n",
    "radius = complex_radius(coefficients)\n",
    "upper_radius = real_radius(coefficients)\n",
    "lower_radius = -real_radius(revert_coefficients)\n",
    "\n",
    "print(f\"Complex radius for roots: {-radius} -> {radius}\")\n",
    "print(\"Upper radius for real-value roots:\", upper_radius)\n",
    "print(\"Lower radius for real-value roots:\", lower_radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tìm các cực trị của đa thức\n",
    "\n",
    "Sử dụng phương pháp Gradient Descent để tìm tất cả các cực trị của hàm đa thức dựa trên các hệ số đầu vào."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thuật toán\n",
    "\n",
    "* B1. Nguyên lý hoạt động\n",
    "\n",
    "    Thuật toán sử dụng phương pháp Gradient Descent/Ascent với tốc độ học thích ứng để tìm các cực trị địa phương của đa thức. Quá trình được thực hiện như sau:\n",
    "\n",
    "    * a. Khởi tạo\n",
    "        - Tạo nhiều điểm khởi đầu phân bố đều trong khoảng `[lower_bound, upper_bound]`\n",
    "        - Mỗi điểm khởi đầu được sử dụng để tìm cả cực đại và cực tiểu\n",
    "        - Khởi tạo tốc độ học ban đầu (`learning_rate`)\n",
    "\n",
    "    * b. Quá trình tìm kiếm\n",
    "        - **Gradient Descent** (tìm cực tiểu):\n",
    "            + Di chuyển ngược hướng gradient\n",
    "            + `x_new = x - learning_rate * gradient`\n",
    "        - **Gradient Ascent** (tìm cực đại):\n",
    "            + Di chuyển cùng hướng gradient\n",
    "            + `x_new = x + learning_rate * gradient`\n",
    "\n",
    "    * c. Cơ chế tốc độ học thích ứng\n",
    "        - Kiểm tra hướng di chuyển:\n",
    "            + Nếu tìm cực đại mà giá trị giảm → sai hướng\n",
    "            + Nếu tìm cực tiểu mà giá trị tăng → sai hướng\n",
    "        - Khi di chuyển sai hướng:\n",
    "            + Giảm tốc độ học một nửa: `learning_rate *= 0.5`\n",
    "            + Thử lại bước di chuyển với tốc độ học mới\n",
    "\n",
    "* B2. Điều kiện dừng và xác nhận cực trị\n",
    "\n",
    "    * a. Điều kiện dừng\n",
    "        - Khoảng cách giữa hai bước liên tiếp nhỏ hơn ngưỡng: `|x_new - x| < tolerance`\n",
    "        - Hoặc đạt số bước lặp tối đa: `max_iterations`\n",
    "\n",
    "    * b. Xác nhận cực trị\n",
    "        - Sử dụng đạo hàm bậc hai để xác nhận:\n",
    "            + Cực tiểu: đạo hàm bậc hai > 0\n",
    "            + Cực đại: đạo hàm bậc hai < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khoảng cách lý nghiệm:\n",
    "\n",
    "- Xếp các `minimum`,`maximum`, `lower_radius`, `upper_radius` thành 1 mảng theo thứ tự tăng dần\n",
    "- Nếu 2 phần tử liên tiếp có giá trị ngược dấu nhau -> *Khoảng cách ly nghiệm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Áp dụng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extrema(coefficients, lower_bound, upper_bound, learning_rate=0.01, max_iterations=1000, tolerance=1e-6, num_starting_points=10):\n",
    "    \"\"\"\n",
    "    Find local minima and maxima using gradient descent/ascent\n",
    "\n",
    "    Variables can be adjusted:\n",
    "    - `coefficients`: hệ số của đa thức\n",
    "    - `learning_rate`: tốc độ học ban đầu\n",
    "    - `tolerance`: ngưỡng hội tụ\n",
    "    - `max_iterations`: số bước lặp tối đa\n",
    "    * `num_starting_points`: số điểm khởi đầu\n",
    "    \"\"\"\n",
    "    starting_points = np.linspace(lower_bound, upper_bound, num_starting_points)\n",
    "    extrema = {'minima': [], 'maxima': []}\n",
    "    \n",
    "    def gradient_search(x0, direction=1):  # direction: 1 for maxima, -1 for minima\n",
    "        x = x0\n",
    "        for _ in range(max_iterations):\n",
    "            gradient = polynomial_derivative(coefficients, x)\n",
    "            x_new = x + direction * learning_rate * gradient\n",
    "            \n",
    "            # Stay within bounds\n",
    "            x_new = np.clip(x_new, lower_bound, upper_bound)\n",
    "            \n",
    "            if abs(x_new - x) < tolerance:\n",
    "                # Check if it's actually an extremum using second derivative\n",
    "                second_deriv = polynomial_second_derivative(coefficients, x_new)\n",
    "                if (direction == -1 and second_deriv > 0) or (direction == 1 and second_deriv < 0):\n",
    "                    return x_new\n",
    "                return None\n",
    "            x = x_new\n",
    "        return None\n",
    "\n",
    "    # Try both gradient descent and ascent from each starting point\n",
    "    for x0 in starting_points:\n",
    "        # Find minimum\n",
    "        min_point = gradient_search(x0, direction=-1)\n",
    "        if min_point is not None:\n",
    "            # Check if this minimum is unique (not already found)\n",
    "            if not any(abs(min_point - x) < tolerance for x in extrema['minima']):\n",
    "                extrema['minima'].append(min_point)\n",
    "        \n",
    "        # Find maximum\n",
    "        max_point = gradient_search(x0, direction=1)\n",
    "        if max_point is not None:\n",
    "            # Check if this maximum is unique (not already found)\n",
    "            if not any(abs(max_point - x) < tolerance for x in extrema['maxima']):\n",
    "                extrema['maxima'].append(max_point)\n",
    "    \n",
    "    # Sort the results\n",
    "    extrema['minima'].sort()\n",
    "    extrema['maxima'].sort()\n",
    "    return extrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extrema(coefficients, lower_bound, upper_bound, learning_rate=0.01, max_iterations=1000, tolerance=1e-6, num_starting_points=10):\n",
    "    \"\"\"\n",
    "    Find local minima and maxima using gradient descent/ascent with adaptive learning rate\n",
    "    \"\"\"\n",
    "    starting_points = np.linspace(lower_bound, upper_bound, num_starting_points)\n",
    "    extrema = {'minima': [], 'maxima': []}\n",
    "    \n",
    "    def gradient_search(x0, direction=1):  # direction: 1 for maxima, -1 for minima\n",
    "        x = x0\n",
    "        current_lr = learning_rate\n",
    "        prev_value = polynomial(coefficients, x)\n",
    "        \n",
    "        for _ in range(max_iterations):\n",
    "            gradient = polynomial_derivative(coefficients, x)\n",
    "            x_new = x + direction * current_lr * gradient\n",
    "            \n",
    "            # Stay within bounds\n",
    "            x_new = np.clip(x_new, lower_bound, upper_bound)\n",
    "            \n",
    "            # Check if we're moving in the wrong direction\n",
    "            current_value = polynomial(coefficients, x_new)\n",
    "            if (direction == 1 and current_value < prev_value) or \\\n",
    "               (direction == -1 and current_value > prev_value):\n",
    "                # We went too far, reduce learning rate and try again\n",
    "                current_lr *= 0.5\n",
    "                continue\n",
    "                \n",
    "            # Update position if improvement is made\n",
    "            if abs(x_new - x) < tolerance:\n",
    "                # Check if it's actually an extremum using second derivative\n",
    "                second_deriv = polynomial_second_derivative(coefficients, x_new)\n",
    "                if (direction == -1 and second_deriv > 0) or (direction == 1 and second_deriv < 0):\n",
    "                    return x_new\n",
    "                return None\n",
    "                \n",
    "            x = x_new\n",
    "            prev_value = current_value\n",
    "            \n",
    "        return None\n",
    "\n",
    "    # Rest of the function remains the same\n",
    "    for x0 in starting_points:\n",
    "        # Find minimum\n",
    "        min_point = gradient_search(x0, direction=-1)\n",
    "        if min_point is not None:\n",
    "            if not any(abs(min_point - x) < tolerance for x in extrema['minima']):\n",
    "                extrema['minima'].append(min_point)\n",
    "        \n",
    "        # Find maximum\n",
    "        max_point = gradient_search(x0, direction=1)\n",
    "        if max_point is not None:\n",
    "            if not any(abs(max_point - x) < tolerance for x in extrema['maxima']):\n",
    "                extrema['maxima'].append(max_point)\n",
    "    \n",
    "    # Sort the results\n",
    "    extrema['minima'].sort()\n",
    "    extrema['maxima'].sort()\n",
    "    return extrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Local minima: ['-3.7767971884620417', '0.8542771897586292', '0.8542865705603361']\n",
      "Local maxima: ['-0.0774904137903172', '-0.07747832449609701']\n",
      "\n",
      "Function values at minima: ['-92.83298523721149', '-1.2066514017780026', '-1.2066514017870618']\n",
      "Function values at maxima: ['1.0396366389480038', '1.0396366389183642']\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "extrema = find_extrema(coefficients, lower_radius, upper_radius)\n",
    "print(\"\\nLocal minima:\", [f\"{x}\" for x in extrema['minima']])\n",
    "print(\"Local maxima:\", [f\"{x}\" for x in extrema['maxima']])\n",
    "\n",
    "# Optionally, print function values at extrema\n",
    "print(\"\\nFunction values at minima:\", [f\"{polynomial(coefficients, x)}\" for x in extrema['minima']])\n",
    "print(\"Function values at maxima:\", [f\"{polynomial(coefficients, x)}\" for x in extrema['maxima']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
